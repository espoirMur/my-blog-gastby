<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8" />
  

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<script
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript">
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin="anonymous" referrerpolicy="no-referrer" />
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="http://localhost:4000/news-summarizer-clustering">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CJV32Y553Z"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CJV32Y553Z');
</script>


  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Clustering french news articles: A case of study of DRCongo news. | Espoir Murhabazi ideas’ home</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Clustering french news articles: A case of study of DRCongo news." />
<meta name="author" content="Murhabazi Buzina Espoir" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="news-summarizer-clustering" />
<meta property="og:description" content="news-summarizer-clustering" />
<link rel="canonical" href="http://localhost:4000/news-summarizer-clustering" />
<meta property="og:url" content="http://localhost:4000/news-summarizer-clustering" />
<meta property="og:site_name" content="Espoir Murhabazi ideas’ home" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-11-06T22:31:32+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Clustering french news articles: A case of study of DRCongo news." />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Murhabazi Buzina Espoir"},"dateModified":"2024-11-06T22:31:32+00:00","datePublished":"2024-11-06T22:31:32+00:00","description":"news-summarizer-clustering","headline":"Clustering french news articles: A case of study of DRCongo news.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/news-summarizer-clustering"},"url":"http://localhost:4000/news-summarizer-clustering"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body>
  <header class="site-header">
  <div class="container">
    <input type="checkbox" id="toggleNavbar">
    <h1 class="logo"><a href="/">esp<span>.py</span></a></h1>
    <label for="toggleNavbar" role="button" class="toggle-navbar-button">
      <i class="icon icon-menu"></i>
      <i class="icon icon-cross"></i>
    </label>
    <nav class="navbar">
      <ul>
        <li><a href="/" title="Home">127.0.0.1</a></li>
        
          <li><a href="/about" title="whoami();">whoami();</a></li>
        
          <li><a href="/blog" title="read();">read();</a></li>
        
          <li><a href="/schedule" title="schedule();">schedule();</a></li>
        
          <li><a href="/learnings" title="learnings();">learnings();</a></li>
        
      </ul>
    </nav>
  </div>
</header>


<main class="main-container">
  <div class="container">
    <article role="article" class="post">

  <div class="card">
    <header class="post-header">
      <h1 class="post-title">Clustering french news articles: A case of study of DRCongo news.</h1>
      <em class="post-meta">
        <time>Nov 6, 2024</time>
      </em>
    </header>

    <div class="post-content">
      
      <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">which</span> <span class="n">python</span>
</code></pre></div></div>

<h1 id="congo-news-summarizer--part-one-news-clustering">Congo News Summarizer : Part one, News Clustering</h1>

<p>Over the past month I have been collecting a lot of news article from major congolese website news webisite. I have those article saved in a postgres database. There are lot of fun stuff I can do with them. Among them there is a news summarizer. I want to analyze the daily news and find out what are the main news the website are talking about.</p>

<p>In this blog or series of post I will try to build that news summarizer. As of now I will structure it as follow:</p>

<p>In the first part, I will talk about how I build the news clustering model, Then in the second part I will talk about how I build an LLM that does the summarization and how I deployed it, finally in the last part I will talk about how to scale the model and deploy it in a production setting.</p>

<p>The end goal of this project is two folds. First, I want  to have a news summarizer that I can open a morning and it will give me a summary of major news that are happening in congo. Second, while building this news summarizer I would like to sharpen my ML Engineering Knowledge and illustrate that I can build an end to end production ready project with the latest python stack.</p>

<p>Let us talk about how I build the clustering model.</p>

<h2 id="data-collection">Data Collection</h2>

<h3 id="data">Data</h3>

<p>I have scrappers that run everyday and scrape the data that congolese news website produce, those article are saved as text in a postgres database.</p>

<p>I  will query that database and load the data in a pandas dataframe for better analysis. I have the code to connect and read from the postgres database embedded in modules.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">dotenv</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">dotenv</span> <span class="p">.</span><span class="o">/</span><span class="p">.</span><span class="n">env_prod</span> <span class="o">-</span><span class="n">o</span>
</code></pre></div></div>

<p>The above line loads the database credentials so that we can query the database.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">src.shared.database</span> <span class="kn">import</span> <span class="n">execute_query</span><span class="p">,</span> <span class="n">generate_database_connection</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">yesterday_article_query</span> <span class="o">=</span> <span class="s">"select content, title, posted_at,url from article where posted_at::date = CURRENT_DATE - interval '1 day'"</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">os</span> <span class="kn">import</span> <span class="n">getenv</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">database_user</span> <span class="o">=</span> <span class="n">getenv</span><span class="p">(</span><span class="s">'POSTGRES_USER'</span><span class="p">)</span>
<span class="n">database_password</span> <span class="o">=</span> <span class="n">getenv</span><span class="p">(</span><span class="s">'POSTGRES_PASSWORD'</span><span class="p">)</span>
<span class="n">database_host</span> <span class="o">=</span> <span class="n">getenv</span><span class="p">(</span><span class="s">'POSTGRES_HOST'</span><span class="p">)</span>
<span class="n">database_port</span> <span class="o">=</span> <span class="n">getenv</span><span class="p">(</span><span class="s">'POSTGRES_PORT'</span><span class="p">)</span>
<span class="n">database_name</span> <span class="o">=</span> <span class="n">getenv</span><span class="p">(</span><span class="s">'POSTGRES_DB'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">database_credentials</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'user'</span><span class="p">:</span> <span class="n">database_user</span><span class="p">,</span>
    <span class="s">'password'</span><span class="p">:</span> <span class="n">database_password</span><span class="p">,</span>
    <span class="s">'host'</span><span class="p">:</span> <span class="n">database_host</span><span class="p">,</span>
    <span class="s">'port'</span><span class="p">:</span> <span class="n">database_port</span><span class="p">,</span>
    <span class="s">'database'</span><span class="p">:</span> <span class="n">database_name</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">connection</span> <span class="o">=</span> <span class="n">generate_database_connection</span><span class="p">(</span><span class="n">database_crendentials</span><span class="o">=</span><span class="n">database_credentials</span><span class="p">)</span>
</code></pre></div></div>

<p>With the credentials, the database connection, the query we can go ahead and query the database to retrieve the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">results</span> <span class="o">=</span><span class="n">execute_query</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">yesterday_article_query</span><span class="p">,</span> <span class="n">database_connection</span><span class="o">=</span><span class="n">connection</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">title</span>
</code></pre></div></div>

<p>We have our results in a list now we can put them in a dataframe from further analysis.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">news_df</span>  <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="n">from_records</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">news_df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">news_df</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span>  <span class="p">[</span><span class="s">"content"</span><span class="p">,</span> <span class="s">"title"</span><span class="p">,</span> <span class="s">"posted_at"</span><span class="p">,</span> <span class="s">"url"</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">current_directory</span> <span class="o">=</span> <span class="n">Path</span><span class="p">.</span><span class="n">cwd</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">news_directory</span> <span class="o">=</span> <span class="n">current_directory</span><span class="p">.</span><span class="n">joinpath</span><span class="p">(</span><span class="s">"datasets"</span><span class="p">,</span> <span class="s">"today_news"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">news_directory</span><span class="p">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">today</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">().</span><span class="n">strftime</span><span class="p">(</span><span class="s">"%Y-%m-%d"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">news_df</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">news_directory</span><span class="p">.</span><span class="n">joinpath</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">today</span><span class="si">}</span><span class="s">-news.csv"</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">news_df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>
<h2 id="preprocessing">Preprocessing.</h2>

<p>In the above code we have collected the data for the previous day, for one day we can have up to 72 news articles written by different news outlets.</p>

<p>We have got our news dataset, we need to now do some preprocessing. The only preprocessing we will do will be to drop the duplicate in the content.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">news_df</span> <span class="o">=</span> <span class="n">news_df</span><span class="p">.</span><span class="n">drop_duplicates</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s">"content"</span><span class="p">).</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">news_df</span>
</code></pre></div></div>

<h2 id="embedding-phase">Embedding phase.</h2>

<p>Machine learning model doesn’t work with text data, we need to convert the text in a representation that machine can understand. To achieve this we need to use embeddings. We will use an  embedding  model to learn representation of our dataset in an embedding space.</p>

<p>We will be using the <code class="language-bash highlighter-rouge">dunzhang/stella_en_400M_v5</code>, it is a good model from huggingface, It offer a good trade-off between the size and performance.  It has a good score on different tasks  in both French and English on the <a href="https://huggingface.co/spaces/mteb/leaderboard">MTEB leaderboard.</a></p>

<p>The bellow code section assume that we have the model downloaded in a locally repository of our machine. If you want do download the model locally you can refer tho this <a href="put the path here">script</a> to learn how to download the model locally.</p>

<p>The code will loads the embedding model and uses it to encode the news. After the encoding we will have for each news article an embedding vector of shape 1024.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">embedding_model_id</span> <span class="o">=</span> <span class="s">"dunzhang/stella_en_400M_v5"</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">current_directory</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_path</span>  <span class="o">=</span> <span class="n">current_directory</span><span class="p">.</span><span class="n">joinpath</span><span class="p">(</span><span class="n">embedding_model_id</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">embedding_model_path</span> <span class="o">=</span> <span class="n">current_directory</span><span class="p">.</span><span class="n">joinpath</span><span class="p">(</span><span class="s">"models"</span><span class="p">,</span> <span class="n">embedding_model_id</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">transformer_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s">"model_name_or_path"</span><span class="p">:</span> <span class="n">embedding_model_path</span><span class="p">.</span><span class="n">__str__</span><span class="p">(),</span>
                      <span class="s">"trust_remote_code"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
                      <span class="s">"device"</span><span class="p">:</span> <span class="s">"cpu"</span><span class="p">,</span>
                      <span class="s">"config_kwargs"</span><span class="p">:</span> <span class="p">{</span><span class="s">"use_memory_efficient_attention"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                                        <span class="s">"unpad_inputs"</span><span class="p">:</span> <span class="bp">False</span><span class="p">},</span>
                      <span class="s">"cache_folder"</span><span class="p">:</span> <span class="n">model_path</span><span class="p">}</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sentence_transformer_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span>
    <span class="o">**</span><span class="n">transformer_kwargs</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">today_news_embeddings</span> <span class="o">=</span> <span class="n">sentence_transformer_model</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">news_df</span><span class="p">.</span><span class="n">content</span><span class="p">.</span><span class="n">tolist</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">today_news_embeddings</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<p>Now we have encoded our news in the embeddings, for each news we have an embedding vector of shape 1024. With those embedding we can now start clustering our news.</p>

<h1 id="clustering">Clustering</h1>

<h2 id="k-means">K-means</h2>

<p>In this step, we will group our news embeddings in a cluster using the K-mean algorithm. The algorithm will try to group the news in clusters based on the similarity of their embedding vectors. After the clustering, we will have similar news grouped in similar clusters. You can learn more about the clustering algorithm <a href="https://www.reddit.com/r/learnmachinelearning/comments/rmx04g/what_is_K-means_clustering_a_2minute_visual_guide/">here</a></p>

<h3 id="how-do-we-pick-the-number-of-cluster">How do we pick the number of cluster?</h3>

<p>One question that is still unclear in the literature about K-mean is how to pick the number of clusters in the K-mean. 
A common approach is to use Silhouette Coefficient. In the next section I will to explain that coefficient.</p>

<h4 id="silhouette-score">Silhouette Score.</h4>

<p>We will use the silhouette score to get the best number of clusters.</p>

<blockquote>
  <p>The Silhouette Coefficient is a measure of how well samples are clustered with samples that are similar to themselves. Clustering models with a high Silhouette Coefficient are said to be dense, where samples in the same cluster are similar to each other, and well separated, where samples in different clusters are not very similar to each other.</p>
</blockquote>

<p>Given the a point $x_i$, and a cluster label $c_i$ to compute the silhouette score:</p>
<ul>
  <li>
    <p>we compute the mean distance of the $x_i$ to all the point in cluster $c_i$, we call it $a_i$</p>

    <table>
      <tbody>
        <tr>
          <td>${\displaystyle a_i={\frac {1}{</td>
          <td>C_{I}</td>
          <td>-1}}\sum <em>{j\in C</em>{I},i\neq j}d(i,j)}$</td>
        </tr>
      </tbody>
    </table>

    <p>Note that we divide by don’t want to include the current point when we are trying to compute the distance.</p>
  </li>
  <li>
    <p>$b_i$ is the a measure to how the point $x_i$ in cluster $c_i$ is dissimilar to all other clusters $c_j$ with $c_j != c_i$.</p>
  </li>
</ul>

<p>For each other clusters different $c_i$ we compute the mean distance between $x_i$ and all the points in the cluster.  Then we take the cluster that has the mean distance as the closest cluster to $x_i$.</p>

<p>We define $b_i$ as:</p>

<table>
  <tbody>
    <tr>
      <td>${\displaystyle b_i=\min _{J\neq I}{\frac {1}{</td>
      <td>C_{J}</td>
      <td>}}\sum <em>{j\in C</em>{J}}d(i,j)}$</td>
    </tr>
  </tbody>
</table>

<p>With those $a_i$, and $b_I$ we define the silhouette score of the point $x_i$ as $s_i$ to be</p>

<p>${\displaystyle s_i={\frac {b_i-a_i}{\max{a_i,b_i}}}}$</p>

<p>This value varies between -1, and 1. 1 means our clusters are dense, and -1 means the opposite.</p>

<p>Let us write a python function that will perform the clustering and return the k that gives us the best cluster.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">silhouette_score</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">find_best_estimator</span> <span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="s">""" compute the k mean clustering, and return the best k that maximize the silhouette score
    """</span>
    <span class="n">k_mean_estimators</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="sa">f</span><span class="s">"KMeans_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">"</span><span class="p">,</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">3000</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">best_estimator</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">best_metric</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">"-inf"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">estimator_name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="n">k_mean_estimators</span> <span class="p">:</span>
        <span class="n">estimator</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">estimator</span><span class="p">.</span><span class="n">labels_</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">silhouette_score</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="n">best_metric</span> <span class="p">:</span>
            <span class="n">best_metric</span> <span class="o">=</span> <span class="n">score</span>
            <span class="n">best_estimator</span> <span class="o">=</span> <span class="n">estimator</span>
        <span class="k">print</span><span class="p">(</span><span class="n">estimator_name</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
        <span class="n">scores</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">best_estimator</span><span class="p">,</span> <span class="n">scores</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_estimator</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">find_best_estimator</span><span class="p">(</span><span class="n">today_news_embeddings</span><span class="p">)</span>
</code></pre></div></div>

<p>In the above function we compute the silhouette score for values for k ranging from 2 to the max number of datapoints in our dataset.</p>

<p>Let plot now the similarity silhouette score and see how it grow with the number of cluster selected.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">today_news_embeddings</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">scores</span><span class="p">)</span>
</code></pre></div></div>
<p>[PUT the image here]</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_estimator</span>
</code></pre></div></div>

<p>We can see that the best estimator gave us the n cluster equal to 29</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">news_df</span><span class="p">[</span><span class="s">"k_means_labels"</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_estimator</span><span class="p">.</span><span class="n">labels_</span>
</code></pre></div></div>

<p>Now let us analyze the clustering results.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">analyse_embeddings</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">label_column</span><span class="o">=</span><span class="s">"labels"</span><span class="p">):</span>
    <span class="s">""" take a matrix of embeddings and the labels.
    for each label compute the cosine similarity of the document with that label.
    """</span>
    <span class="n">document_in_index</span> <span class="o">=</span> <span class="n">dataframe</span><span class="p">.</span><span class="n">query</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">label_column</span><span class="si">}</span><span class="s"> == </span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">pd</span><span class="p">.</span><span class="n">option_context</span><span class="p">(</span><span class="s">'display.max_colwidth'</span><span class="p">,</span> <span class="bp">None</span><span class="p">):</span>
        <span class="n">display</span><span class="p">(</span><span class="n">document_in_index</span><span class="p">.</span><span class="n">title</span><span class="p">)</span>
    <span class="n">document_index</span> <span class="o">=</span> <span class="n">document_in_index</span><span class="p">.</span><span class="n">index</span>
    <span class="n">vectors</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">document_index</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">sentence_transformer_model</span><span class="p">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span>  <span class="n">vectors</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">analyse_embeddings</span><span class="p">(</span><span class="n">news_df</span><span class="p">,</span> <span class="n">today_news_embeddings</span><span class="p">,</span><span class="mi">29</span><span class="p">,</span> <span class="n">label_column</span><span class="o">=</span><span class="s">"k_means_labels"</span><span class="p">)</span>
</code></pre></div></div>

<p>After the first look at the results we can see that the results are good, we have around 29 news cluster, for 72 news.
Some news cluster have only one article which make sense, and other have up to 6 articles. In the later analysis we will only keep news articles that have more than one documents.</p>

<p>Can we do better than that? Let now try hierarchical clustering</p>

<h2 id="hierarchical-clustering">Hierarchical Clustering</h2>

<p>Hierarchical clustering is a clustering that uses an iterative approach to build the <code class="language-bash highlighter-rouge">dendrogram</code>.</p>

<p>A dendrogram is a representation of a tree. In hierarchical clustering, it illustrates the arrangement of the clusters produced by the corresponding analyses.</p>

<p><strong>How do we build a dendrogram?</strong></p>

<p>Assuming we have <code class="language-bash highlighter-rouge">n</code> points that we would like to cluster, the algorithm starts with them and a similarity metric to use.
In the first step, all the <code class="language-bash highlighter-rouge">n</code> points are grouped in a <code class="language-bash highlighter-rouge">n </code> clusters, as each observation is treated as a separate cluster.
Then, the next two similar clusters are fused into a cluster; at this point, we have <code class="language-bash highlighter-rouge">n-1</code> clusters.
The algorithm will process iteratively  by fusing similar clusters into each other until we have one cluster.<br />
With one cluster we have our dendrogram complete.</p>

<p>In the figure, [Put the figure here], illustrate a dendrogram resulting from the clustering.</p>

<p><strong>How do we compute similarity between clusters?</strong></p>

<p>We have the notion between similarity between two points but how do we compute the similarity between a point and a cluster or between two clusters?
The notion of similarity between two points can be extend to develop the notion of <code class="language-bash highlighter-rouge">linkage</code> which is how we evaluate the similarity between two groups of observation or clusters.
Given two clusters A and, linkage metrics start by computing the pairwise  dissimilarity between the observations in cluster A and those in cluster B.</p>

<p>Depending on how we will compute the overall dissimilarity from those pairwise dissimilarities, the linkage metric will be defined.</p>

<p>The linkage is called:</p>

<ul>
  <li>
    <p><strong>complete</strong>: When overall dissimilarity is the largest of the pairwise dissimilarity.</p>
  </li>
  <li>
    <p><strong>single</strong>:  When overall dissimilarity is the smallest of the pairwise dissimilarity.</p>
  </li>
  <li>
    <p><strong>average</strong>: When overall dissimilarity is the average of the pairwise dissimilarity.</p>
  </li>
</ul>

<p>With the understanding of of Hierarchical clustering and the linkage metric, let implement hierarchical clustering using the <code class="language-bash highlighter-rouge">scipy package</code>.
As the result of the hierarchical clustering is a tree, which can be visualized as a dendrogram.</p>
<h3 id="hierarchical-clustering-with-scipy">Hierarchical Clustering with Scipy.</h3>

<p>To implement the the hierarchical clustering, we will use two functions from the <code class="language-bash highlighter-rouge">scipy</code> package, the <code class="language-bash highlighter-rouge">linkage</code> and the <code class="language-bash highlighter-rouge">dendrogram</code> function.</p>

<p>The linkage function perform the clustering, it takes the input embeddings a a numpy array, the linkage method, and the similarity metric and it return the hierarchical clustering tree encoded as a linkage matrix.</p>

<p>We use the <code class="language-bash highlighter-rouge">dendrogram</code> function to generate the tree plot of the linkage matrix.
The bellow code illustrate how the clustering is performed.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">dendrogram</span><span class="p">,</span> <span class="n">linkage</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Complete Linkage
</span><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">mergings</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">today_news_embeddings</span><span class="p">,</span>
                   <span class="n">method</span><span class="o">=</span><span class="s">'complete'</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s">'cosine'</span><span class="p">)</span>
<span class="n">dendrogram</span><span class="p">(</span><span class="n">mergings</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p>[PUT THe Image of the dendrogram here]</p>

<p>On the above plot, the x axis represent the document which are group into cluster based on th color, the y axis represent the distance cut-off use while computing the merging.</p>

<p>On the <em>y</em> axis represent the distance cut-off use while computing the merging.
On the <em>x</em> axis represent the document which are group into cluster based on th color.</p>

<p><strong>A quick note on the merging value</strong>:</p>

<p>Each row of the merging matrix is in the format [cluster_index, cluster_index, distance, sample_count], the colum index are the ith iteration at which those merging was done.</p>

<p>Recall that we said that the hierarchical clustering consider each point as a separate cluster to start with and then iteratively merge those points two by two to create new clusters.</p>

<p>Let have a look at what happen in the first 8 iteration of our algorithm.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import numpy as np
mergings[:, 2] <span class="o">=</span> 1 - mergings[:, 2] <span class="c"># the distance is 1 - the cosine similarity</span>
with np.printoptions<span class="o">(</span><span class="nv">precision</span><span class="o">=</span>3, <span class="nv">suppress</span><span class="o">=</span>True<span class="o">)</span>:
    print<span class="o">(</span>mergings[:9]<span class="o">)</span>
</code></pre></div></div>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[[</span> 9.    40.     0.927  2.   <span class="o">]</span>
 <span class="o">[</span>19.    72.     0.918  2.   <span class="o">]</span>
 <span class="o">[</span> 8.    42.     0.906  2.   <span class="o">]</span>
 <span class="o">[</span>34.    71.     0.898  2.   <span class="o">]</span>
 <span class="o">[</span>27.    52.     0.889  2.   <span class="o">]</span>
 <span class="o">[</span>17.    49.     0.887  2.   <span class="o">]</span>
 <span class="o">[</span>25.    47.     0.882  2.   <span class="o">]</span>
 <span class="o">[</span>30.    81.     0.878  3.   <span class="o">]</span>
 <span class="o">[</span>44.    80.     0.869  3.   <span class="o">]]</span>
</code></pre></div></div>
<p>In the above matrix we can see that the document with index 9, 40 and the similarity distance of those document: 0.92.</p>

<p>Let see how what is inside those two documents:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>with pd.option_context<span class="o">(</span><span class="s1">'display.max_colwidth'</span>, None<span class="o">)</span>:
        display<span class="o">(</span>news_df.iloc[[9, 40]].title<span class="o">)</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>9     RD -États généraux de la justice : “Un minimum de volonté de nous tous suffit pour que nous ayons un appareil judiciaire fort” <span class="o">(</span>Félix Tshisekedi<span class="o">)</span>
40                              Félix Tshisekedi lançant les états généraux de la Justice : « Notre appareil judiciaire sera restauré bon gré mal gré »
Name: title, dtype: object
</code></pre></div></div>

<p>We can check other documents in the merging to understand how the merging work. When we get to the 9th iteration we can see that it merged 3 samples and the second cluster index  at that iteration is not a single document, it index is 80 which is greater than the number of documents we have in our set(79). That mean that the cluster 44 which has only one document, the document 44 was merged with the cluster 80 which contain 2 documents. They were merged at a distance of 0.878.</p>

<p>The cluster 80 is build with documents merged together in the <code class="language-bash highlighter-rouge">80 - len<span class="o">(</span>document<span class="o">)</span>th</code> iteration. In our case it represent the documents merged in the 1st iteration(80 - 79). From the above matrix we can see that in the first iteration the documents 9 and 40 were merged together. With that we can say that at the 9th iteration the three documents that were merged together are documents with id: 9, 40, and 44. Let see how those document look like from our set.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>with pd.option_context<span class="o">(</span><span class="s1">'display.max_colwidth'</span>, None<span class="o">)</span>:
        display<span class="o">(</span>news_df.iloc[[9, 40, 44]].title<span class="o">)</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>9     RD -États généraux de la justice : “Un minimum de volonté de nous tous suffit pour que nous ayons un appareil judiciaire fort” <span class="o">(</span>Félix Tshisekedi<span class="o">)</span>
40                              Félix Tshisekedi lançant les états généraux de la Justice : « Notre appareil judiciaire sera restauré bon gré mal gré »
44                                                                                      La justice congolaise malade, son traitement débute aujourd<span class="s1">'hui
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">fcluster</span>
</code></pre></div></div>

<h3 id="selecting-the-cluster-labels-for-document">Selecting the cluster labels for document</h3>

<p>If we pick a distance metric 
from the linkage matrix we can return the label using a metric cut-off.</p>

<p>How do we select the best metric cut-off to use for the clustering?
We will use the silhouette score and the do the same approach we used with the K-mean. We will select the metric that gives us a high silhouette score.</p>

<p>The <code class="language-bash highlighter-rouge">fcluster</code> function helps take a distance cutt-off and return the cluster label of each document the value of k as distance cutt off. So if we say k = 0.2, the function will give us the clustering of the document assuming that max distance of the document in a cluster is 0.2.</p>

<p>But how do we find the best k to select?</p>

<ul>
  <li>We can use domain knowledge or we can fine tune that metric. We have decided to fine tune, and by fine tuning we select the k that gives us the best silhouette score.</li>
</ul>

<p>The below code perform the fine tuning and return the k that maximize the silhouette score.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">select_best_distance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">merging</span><span class="p">):</span>
    <span class="s">""" 
    Start with the document embedding x, and the hierarchical clustering, 
    find the k that maximize the max_silhouette score
    """</span>
    <span class="n">max_max_silhouette</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">"-inf"</span><span class="p">)</span>
    <span class="n">return_labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">number_of_clusters</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">best_k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">):</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">fcluster</span><span class="p">(</span><span class="n">merging</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s">"distance"</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">silhouette_score</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">labels</span>
        <span class="p">)</span>
        <span class="n">scores</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="n">n_clusters</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">labels</span><span class="p">).</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">number_of_clusters</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="n">max_max_silhouette</span><span class="p">:</span>
            <span class="n">max_max_silhouette</span> <span class="o">=</span> <span class="n">score</span>
            <span class="n">return_labels</span> <span class="o">=</span> <span class="n">labels</span>
            <span class="n">best_k</span> <span class="o">=</span> <span class="n">k</span>
    <span class="k">return</span> <span class="n">scores</span><span class="p">,</span> <span class="n">return_labels</span><span class="p">,</span> <span class="n">number_of_clusters</span><span class="p">,</span> <span class="n">best_k</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">scores</span><span class="p">,</span> <span class="n">label_hierarchical</span><span class="p">,</span> <span class="n">number_of_clusters</span><span class="p">,</span> <span class="n">best_k</span> <span class="o">=</span>  <span class="n">select_best_distance</span><span class="p">(</span><span class="n">today_news_embeddings</span><span class="p">,</span> <span class="n">mergings</span><span class="p">)</span>
</code></pre></div></div>
<p>Which method do I prefer, at this point the automatic method does not work in most of the case. However setting the best metric to a value equal to k=0.3 seems to give me the best cluster. With my embedding most document which has a cosine similarity of 1-0.3 = 0.7 seems to be related with that cut-off distance the cluster are meaningful.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span> <span class="n">scores</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Distance metric"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"silhouette score"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"silhouette score vs distance metric"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">label_hierarchical</span><span class="p">).</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">max</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">best_k</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span> <span class="n">number_of_clusters</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Distance metric"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"silhouette score"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"distance vs number of clusters"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">news_df</span><span class="p">[</span><span class="s">"label_hierarchical"</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_hierarchical</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">news_df</span><span class="p">.</span><span class="n">query</span><span class="p">(</span><span class="s">"label_hierarchical == 1"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">analyse_embeddings</span><span class="p">(</span><span class="n">news_df</span><span class="p">,</span> <span class="n">today_news_embeddings</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s">"label_hierarchical"</span><span class="p">)</span>
</code></pre></div></div>

<p>Once i have got the best labeling, i can go ahead and select the most important cluster.</p>

<p>This will be all the cluster with more than 1 document, the rest of the document will be considered as noise. Each cluster with one document will be considered as noise.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cluster_counts</span> <span class="o">=</span> <span class="n">news_df</span><span class="p">.</span><span class="n">label_hierarchical</span><span class="p">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">labels_with_more_than_one</span> <span class="o">=</span> <span class="n">cluster_counts</span><span class="p">[</span><span class="n">cluster_counts</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">].</span><span class="n">index</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">important_news_df</span> <span class="o">=</span> <span class="n">news_df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">news_df</span><span class="p">.</span><span class="n">label_hierarchical</span><span class="p">.</span><span class="n">isin</span><span class="p">(</span><span class="n">labels_with_more_than_one</span><span class="p">)]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">important_news_df</span><span class="p">.</span><span class="n">label_hierarchical</span><span class="p">).</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">important_news_df</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">news_directory</span><span class="p">.</span><span class="n">joinpath</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">today</span><span class="si">}</span><span class="s">-important-news-clusters.csv"</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="conclusion">Conclusion</h3>

<p>In this notebook I went trough the process of building a news clustering system. We started by pulling the data form the database, then we computed the news embedding using the embedding model. With the embedding vectors of the news, we started the clustering. We explain the silhouette score, which is the metric we use to evaluate the quality of clusters resulting from a clustering algorithm, then we explained and performed hierarchical clustering on our news embeddings. At the end of the hieararchical clustering we end up with news clusters finally we saved those data in a file for further analysis and downstream applications.</p>

<p>In the next post, we will move from the jupyter notebook to a production ready application. We will learn how to productionarize this simple news clustering system. Stay tunned for that post.</p>

<p>At this point we have a notebook with the clustering results and those results are saved back in the folder. The next step will be to build an news cluster component that will be use in a downstream application.</p>

<p>Reference:</p>

<ul>
  <li>Article on The clustering : https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/</li>
  <li>Machine Learning book on Clustering</li>
  <li>Wikipedia on the clustering</li>
</ul>

    </div>

    
<hr>

<aside id="comments" class="disqus">
  <h3><i class="icon icon-comments-o"></i> Comments</h3>
  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function() {
      this.page.url = 'http://localhost:4000/news-summarizer-clustering';
      this.page.identifier = '/news-summarizer-clustering';
    };
    (function() {
      var d = document,
      s = d.createElement('script');
      s.src = 'https://espoirmurhabazi.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</aside>


  </div>

</article>

  </div>
</main>

<footer class="site-footer">
  <div class="container">
    <ul class="social">
  <li><a href="" target="_blank"><i class="icon icon-github"></i></a></li>
  <li><a href="esp_py" target="_blank"><i class="icon icon-twitter"></i></a></li>
  <li><a href="" target="_blank"><i class="icon icon-linkedin"></i></a></li>
  <li><a href="" target="_blank"><i class="fa-brands fa-stack-overflow"></i></a></li>
</ul>

    <p class="txt-medium-gray">
      <small>&copy;2024 All rights reserved. Made with <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> and ♥</small>
    </p>
  </div>
</footer>


</body>
</html>
